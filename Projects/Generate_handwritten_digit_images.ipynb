{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ey_BtgSkoPIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc3e2a0a-7acf-4b33-a972-d684b91c0c34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.34.2)\n",
            "Requirement already satisfied: tensorflow-docs in /usr/local/lib/python3.10/dist-packages (2024.2.5.73858)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs) (0.8.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs) (3.1.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs) (5.10.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from tensorflow-docs) (6.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->tensorflow-docs) (2.1.5)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat->tensorflow-docs) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs) (0.20.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->tensorflow-docs) (4.3.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow imageio tensorflow-docs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "QSgTMrH8oeqk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "RKh38Vr_opXV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pgtd7pnTorUO",
        "outputId": "750f4f8b-5139-49f3-9d28-2346763a585f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]"
      ],
      "metadata": {
        "id": "5SZZ2CSpotb1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256"
      ],
      "metadata": {
        "id": "xaz9Fpj5ovVx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "AabTKsbDoxPi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise = tf.random.normal([1, 100])"
      ],
      "metadata": {
        "id": "h-koxwOTo0Pe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(noise[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anrxv8qio1ui",
        "outputId": "70b32563-094d-4c0c-a8f0-598fe5d8aada"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the Model<br> <br>\n",
        "Generator\n"
      ],
      "metadata": {
        "id": "BJ4X-hi_o3uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "frhxZt08pA7S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the untrained generator to generate an image from random noise\n",
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "llwpljoHpEOq",
        "outputId": "0d397431-3d30-4b46-8f67-b86a6aa566f4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7b172b1369b0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoqUlEQVR4nO3de3CV9Z3H8U8SkhMuyQkh5EYuJFFAQYIiRApClCwXVytKu2rdXWgdHGywVXqlF62tnVQcrbXLyrTbldVRqToioi2sogmrXFxuRloMlwkkQBIkJQkk5AJ59g+GbCO3fB8Tfkl8v2bODEmeT54fz3lyPjk553xPiOd5ngAAuMRCXS8AAPDFRAEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcKKP6wV8Vmtrqw4dOqSoqCiFhIS4Xg4AwMjzPB07dkzJyckKDT3//ZxuV0CHDh1Samqq62UAAD6n8vJypaSknPfr3a6AoqKiJEnz589XIBDocM6y7RnHjx83Z/xqaGgwZwYPHmzOVFdXmzN+72nGxsaaM3V1db72ZeX3um1qajJnRo0aZc6UlpaaM4MGDTJndu3aZc5IUkJCgjnj53wtLy83Z/wcBz/nquTvfK2srDRngsGgOdO/f39zRpKOHTtmzpw8edK0fXNzs55//vm22/Pz6bICWrJkiR5//HFVVlYqOztbv/3tbzV+/PiL5s7cGAYCgS4voJaWFnPGL+sVKPn7P0VERJgzfgvIz/r8ZPxobm72lWttbTVnIiMjzRk/15OfYxceHm7OSJdufZdqP36uI8nfLySX6v/k92fJz//pQn9Gu5CL3bZ0yZMQ/vjHP2rhwoV6+OGHtXXrVmVnZ2v69Ok6fPhwV+wOANADdUkBPfnkk5o3b56+/vWv68orr9TSpUvVr18//ed//mdX7A4A0AN1egE1Nzdry5YtysvL+/+dhIYqLy9PGzZsOGv7pqYm1dXVtbsAAHq/Ti+gI0eO6NSpU2c9iJmQkHDOB+cKCgoUDAbbLjwDDgC+GJy/EHXRokWqra1tu/h5VgwAoOfp9GfBxcXFKSwsTFVVVe0+X1VVpcTExLO2tz7bDQDQO3T6PaCIiAiNHTtWa9eubftca2ur1q5dqwkTJnT27gAAPVSXvA5o4cKFmjNnjq699lqNHz9eTz31lOrr6/X1r3+9K3YHAOiBuqSA7rjjDn366ad66KGHVFlZqTFjxmj16tW+Xl0NAOidumwSwoIFC7RgwYKu+vZn8TPqJi0tzde+KioqzBk/z+5rbGw0Z/yM9PD7GJyfsT/nehzwYi42zuNcSkpKzBlJ+vKXv2zObNu2zZwZMGCAObN9+3ZzZurUqeaMdPrZrFYZGRnmjJ9jN27cOHOmrKzMnJH8TcZIT083Z4YNG2bOvPfee+aM5G/MknW0led5HdrO+bPgAABfTBQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwosuGkX5eYWFhCgsL6/D2LS0t5n3s37/fnJGkkJAQc+bAgQPmTEREhDnjZ8hlUVGROSNJt99+uzlTXFxszkyePNmcycnJMWckafXq1eaMn0GSfobnjhkzxpzxc7wlf8dv586d5sxXvvIVc+ajjz4yZw4ePGjOSNKQIUPMmebmZnPmd7/7nTlz8803mzOSVF9fb85ER0ebtm9qaurQdtwDAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBPddhp2dXW1aRr0FVdcYd5HTU2NOSNJjY2N5szQoUPNmbq6OnPm2muvNWf8TO+VpMjISHNm4sSJ5ozneebM2rVrzRlJqqysNGfS0tLMmfT0dHOmurranBk3bpw5I0kZGRnmzCeffGLOvPHGG+bMl770JXMmMzPTnJGkuLg4c+att94yZ/xMOi8vLzdnJH+3Rdu3bzdt39HbFO4BAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIAT3XYY6eDBgxUIBDq8/e7du837CAaD5owktba2mjM7d+40Z5qamsyZpKQkc6ahocGckaSqqipzxnKdnuFnWGpCQoI5I0ktLS3mTGlpqTnjZ9Bs//79zRk/g3MlfwMr/Qww7du3rznz8ccfmzOjR482ZyTp4MGD5kxqaqo54+c2JSYmxpyRpPDwcHPGej2FhYV1aDvuAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAE912GGltba0iIiI6vP2AAQPM+wgJCTFnJH/D/MaNG2fOrF+//pJkxo8fb85IUlZWljlz8uRJcyY01P570tatW80ZSfrXf/1Xc8bzPHOmsLDQnHnrrbfMmccee8yckaQ1a9aYM0OGDDFn9u/fb848+uij5szy5cvNGcnfcN/Vq1ebM7fccos5U1RUZM5IUnZ2tjlTXV1t2r6jQ325BwQAcIICAgA40ekF9LOf/UwhISHtLiNGjOjs3QAAergueQxo5MiReuedd/5/J3267UNNAABHuqQZ+vTpo8TExK741gCAXqJLHgPavXu3kpOTlZmZqbvvvltlZWXn3bapqUl1dXXtLgCA3q/TCygnJ0fLli3T6tWr9cwzz6i0tFTXX3+9jh07ds7tCwoKFAwG2y5+3k8dANDzdHoBzZw5U1/96lc1evRoTZ8+XX/6059UU1Ojl19++ZzbL1q0SLW1tW2X8vLyzl4SAKAb6vJnB8TExGjYsGHas2fPOb8eCAQUCAS6ehkAgG6my18HdPz4ce3du9fXK4oBAL1XpxfQd7/7XRUVFWnfvn1av369brvtNoWFhemuu+7q7F0BAHqwTv8T3IEDB3TXXXepurpagwcP1qRJk7Rx40YNHjy4s3cFAOjBOr2A/A79+6wBAwZ0+WNDffv29ZU7cOCAOePn/xIZGWnO+HnRr9+hhpmZmebM008/bc5861vfMmd27dplzkjy9TIAP8fczxDO6dOnmzOrVq0yZyTp1KlT5oyfIb0pKSnmzNKlS82ZWbNmmTOS9MYbb5gz8+fPN2f8DDCNi4szZyTpyJEj5szll19u2r6pqalD2zELDgDgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCc6PI3pPOrrq5OERERHd7ez1t5W77/3zvf24tfSG1trTnzL//yL+bMf//3f5szft+rKTEx0Zz553/+Z3Pmo48+MmcmT55szkjStm3bzJknnnjCnHn00UfNmdLSUnMmIyPDnJGktLQ0c2bz5s3mTENDgzkzcOBAc+att94yZyRpxIgR5sz69evNmbFjx5ozQ4cONWck6fDhw+bMu+++a9q+ubm5Q9txDwgA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOdNtp2MFgUIFAoMPb79q1y7wPPxN/JSkyMtKc2b59uznjZ/pxVlaWORMMBs0ZSVqyZIk5Y7lOz/CzPj/XkSSFhYWZM7/61a/MGT9ToBcvXmzO/PrXvzZnJOnTTz81Z3Jzc82ZAwcOmDN//vOfzZl58+aZM5K/9VVXV5szx48fN2cGDx5szkj+rlvrz1NoaMfu23APCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCc6LbDSBMTE9W3b98Ob9/Y2GjeR0pKijkjSa2treZMYmKiOTNhwgRzZv/+/eaM36GGP/zhD82Z+fPnmzOTJk0yZy677DJzRpKio6PNmWHDhpkzr7zyijmzfPlyc2bs2LHmjCStWbPGnGlpaTFnZs2aZc5ceeWV5ozf4bR+hsb6GWDq53ytr683ZyR/g5Hj4+NN2/fp07Fq4R4QAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADjRbYeRlpeXKxAIdHj78PBw8z6OHj1qzkjS+++/b86MGTPGnDl48KA58/bbb5szfoY7SlJNTY05c/jwYXPm9ddfN2dSU1PNGUmKiIgwZ0JD7b/HxcXFmTO1tbXmjN9z/Bvf+IY58/HHH5szfgbafvvb3zZnXnvtNXNG8vezkZCQYM7s3bvXnKmrqzNnJGnkyJHmzIABA0zbnzhxokPbcQ8IAOAEBQQAcMJcQOvWrdMtt9yi5ORkhYSEnPXnEc/z9NBDDykpKUl9+/ZVXl6edu/e3VnrBQD0EuYCqq+vV3Z2tpYsWXLOry9evFhPP/20li5dqk2bNql///6aPn26rzeMAwD0XuYnIcycOVMzZ84859c8z9NTTz2ln/zkJ7r11lslSc8995wSEhL0+uuv68477/x8qwUA9Bqd+hhQaWmpKisrlZeX1/a5YDConJwcbdiw4ZyZpqYm1dXVtbsAAHq/Ti2gyspKSWc/DTEhIaHta59VUFCgYDDYdvH79FkAQM/i/FlwixYtUm1tbdulvLzc9ZIAAJdApxZQYmKiJKmqqqrd56uqqtq+9lmBQEDR0dHtLgCA3q9TCygjI0OJiYlau3Zt2+fq6uq0adMmTZgwoTN3BQDo4czPgjt+/Lj27NnT9nFpaam2b9+u2NhYpaWl6YEHHtCjjz6qyy+/XBkZGfrpT3+q5ORkzZo1qzPXDQDo4cwFtHnzZt1www1tHy9cuFCSNGfOHC1btkzf//73VV9fr3vvvVc1NTWaNGmSVq9ercjIyM5bNQCgxwvxPM9zvYi/V1dXp2AwqHvuucc0GNLPtIXrrrvOnJGkkydPmjP9+/c3Z1JSUsyZl19+2Zzx66677jJnLrvsMnNm06ZN5szAgQPNGUm+XjA9fvx4c+bJJ580Z8aNG2fOFBcXmzOSlJuba85kZWWZM6tWrTJn/AxyjYmJMWckqaSkxJzp16+fOdPU1GTO/NM//ZM5I0lbtmwxZ/7+YZWOaGlp0auvvqra2toLPq7v/FlwAIAvJgoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJzottOw77//fgUCgQ7namtrzfsaOnSoOSNJR44cMWdaWlrMmczMTHMmIyPDnHnjjTfMGUkKDbX//rJv3z5z5p577jFn/Ey1lqS//OUv5oyf6+nUqVPmjJ9zfNeuXeaM5G9qeXNzszmTmppqznz00UfmzPDhw80ZSRo0aJA585vf/Mac8TPpvKGhwZyRTt/GWlnX19jYqJ///OdMwwYAdE8UEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcKKP6wWcT0NDg06ePNnh7S3bnuFnQKgkRUREmDOtra3mzM6dO80ZP4MG4+LizBnJ38DKPn3sp9yhQ4fMGT9DLiUpPT3dnCktLTVnkpOTzZn169ebM0888YQ5I0k33XSTOXPDDTeYM7/85S/NmXXr1pkzTU1N5owkffDBB+aMn4HAt912mznz6aefmjOSlJCQYM6sWrXKtH1Hjzf3gAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADAiW47jDQYDCoQCHR4+9BQe5f6HVA4aNCgS7KvsWPHmjN+BjXGx8ebM5K0efNmc2bSpEnmzKlTp8yZ//mf/zFnJCktLc1XzsrP8Fw/x+E3v/mNOSNJWVlZ5oyfn8H/+I//MGeefvppc2bKlCnmjCRVVVWZM7m5uebMn//8Z3PGLz/DUq1DhDt6rnIPCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCc6LbDSMPDwxUeHt7h7Y8ePWreh5+hfJK/gZ8TJ040Zw4cOGDODB8+3JwpKioyZyQpLi7OnGlpaTFn3nnnHXMmOjranJGkv/3tb+bMuHHjzJnnn3/enLnxxhvNmZiYGHNG8nfdRkZGmjN+Btrm5OSYM9XV1eaMJN18883mzEcffWTO+BkIXFdXZ85I0gcffGDOWH+empubO7Qd94AAAE5QQAAAJ8wFtG7dOt1yyy1KTk5WSEiIXn/99XZfnzt3rkJCQtpdZsyY0VnrBQD0EuYCqq+vV3Z2tpYsWXLebWbMmKGKioq2y0svvfS5FgkA6H3MT0KYOXOmZs6cecFtAoGAEhMTfS8KAND7dcljQIWFhYqPj9fw4cN13333XfAZKE1NTaqrq2t3AQD0fp1eQDNmzNBzzz2ntWvX6rHHHlNRUZFmzpx53vcILygoUDAYbLukpqZ29pIAAN1Qp78O6M4772z791VXXaXRo0crKytLhYWFmjp16lnbL1q0SAsXLmz7uK6ujhICgC+ALn8admZmpuLi4rRnz55zfj0QCCg6OrrdBQDQ+3V5AR04cEDV1dVKSkrq6l0BAHoQ85/gjh8/3u7eTGlpqbZv367Y2FjFxsbqkUce0ezZs5WYmKi9e/fq+9//vi677DJNnz69UxcOAOjZzAW0efNm3XDDDW0fn3n8Zs6cOXrmmWdUXFys//qv/1JNTY2Sk5M1bdo0/eIXv1AgEOi8VQMAejxzAeXm5srzvPN+fc2aNZ9rQWc0NzcrJCSkw9ufPHnSvA8/GUkaMmSIOeNn6GJ6ero5M2XKFHMmNzfXnJGkY8eOmTPLli0zZ/r0sT9X5rrrrjNnJH/DSP28dKB///7mzMcff2zOXMrHVIcNG3ZJ9vOnP/3JnPnqV7/qa19VVVXmTFhYmDmzb98+c+ZCt8MXMmbMGHPmxIkTpu2bmpo6tB2z4AAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOBEp78ld2cJCQkxTcM+evSoeR/XXHONOSN1fNLr3/PzNuNXX321ObN8+XJzZt26deaMJD3xxBPmjJ+pxDU1NeaMn+MgSb/73e/MmR//+MfmzNy5c80ZPxPVp02bZs5I0v79+82ZV1991ZzJysoyZ773ve+ZMykpKeaMJP3iF78wZ8rKysyZGTNmmDNxcXHmjORvirb1Z5Bp2ACAbo0CAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATnTbYaShoaEKDe14Pw4aNMi8j4MHD5ozkvT++++bM//wD/9gzixdutScGT16tDmTm5trzkjS1q1bzZkvfelL5oyf62nAgAHmjCRt2bLFnPFz/JYtW2bODBs2zJx54403zBlJSkxMNGeioqLMmcjISHOmqKjInFm5cqU5I0m33nqrOTNp0iRzpri42Jzp08ffzfcnn3xizmRkZJi2b21t7dB23AMCADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACe67TDSxsZGeZ7X4e2Tk5PN+/AzwFSSvvzlL5szAwcONGfmzJljzhw+fNicaW5uNmckKTs725yprKw0ZxoaGsyZ2bNnmzOS9NRTT5kzU6ZMMWdGjRplzvgZEBodHW3OSNLmzZvNmZtuusmcKS0tNWeSkpLMmaFDh5ozkr/r6bXXXjNnbr/9dnNm+PDh5owkhYeHmzPW28oTJ050aDvuAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAE912GGlISIhCQkI6vL2fgZr19fXmjORv4GdkZKQ58/vf/96ceeyxx8yZo0ePmjOSVFVVZc7ExcWZM++99545k5GRYc5I0jXXXGPO+BkKWV1dbc5s27bNnOnTx9+PuJ+BlVu3bjVnRo4cac688MIL5oyf61WS3nrrLXPmK1/5ijnj5xxPT083ZyRpyJAh5kxxcbFp+6ampg5txz0gAIATFBAAwAlTARUUFGjcuHGKiopSfHy8Zs2apZKSknbbNDY2Kj8/X4MGDdKAAQM0e/ZsX3+qAQD0bqYCKioqUn5+vjZu3Ki3335bLS0tmjZtWrvHUh588EGtWrVKr7zyioqKinTo0CFfb7YEAOjdTI9Qrl69ut3Hy5YtU3x8vLZs2aLJkyertrZWf/jDH/Tiiy/qxhtvlCQ9++yzuuKKK7Rx40Zdd911nbdyAECP9rkeA6qtrZUkxcbGSpK2bNmilpYW5eXltW0zYsQIpaWlacOGDef8Hk1NTaqrq2t3AQD0fr4LqLW1VQ888IAmTpzY9r7plZWVioiIUExMTLttExISVFlZec7vU1BQoGAw2HZJTU31uyQAQA/iu4Dy8/O1Y8cOLV++/HMtYNGiRaqtrW27lJeXf67vBwDoGXy9Sm3BggV68803tW7dOqWkpLR9PjExUc3NzaqpqWl3L6iqqkqJiYnn/F6BQECBQMDPMgAAPZjpHpDneVqwYIFWrFihd99996xXm48dO1bh4eFau3Zt2+dKSkpUVlamCRMmdM6KAQC9gukeUH5+vl588UWtXLlSUVFRbY/rBINB9e3bV8FgUPfcc48WLlyo2NhYRUdH6/7779eECRN4BhwAoB1TAT3zzDOSpNzc3Haff/bZZzV37lxJ0q9//WuFhoZq9uzZampq0vTp0/Xv//7vnbJYAEDvYSogz/Muuk1kZKSWLFmiJUuW+F6UJFVUVJgGIvoZNvjJJ5+YM5K/AY8nTpwwZ8aPH2/O7Nu3z5zxM1xVksaMGWPO7Ny505zJzMw0Z7Kzs80ZSb7+VHzq1Clzxs+TbdasWWPOnPnF0MrP47JnXpZh4Wco61133WXOnO9ZuBfTr18/c2bXrl3mzJmXsljU1NSYM5L0wQcfmDOffWbzxYSFhXVoO2bBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAlf74h6KaSkpJgm8jY3N5v3cf3115szkr+JzvX19eaMnwm+H374oTkTDAbNGUn6+OOPzZnk5GRzpqKiwpx5/vnnzRlJmj17tjkTGRlpzuTk5JgzfviZ5ixJCQkJ5oyf4+Dn56Kjk5b/3tatW80Z6fSbbFr5Wd8VV1xhzjz33HPmjCRde+215sz+/ftN23f09ph7QAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgRLcdRvrpp58qIiKiw9tbtj2jvLzcnJEu3QBFP8NS4+PjzZnjx4+bM5J08uRJc6awsNCcCQ8PN2eGDh1qzkhSVFSUOVNWVmbOjBo1ypy5++67zZnNmzebM5I0aNAgc6alpcWc8XM9HTp0yJwZPXq0OSNJf/vb38yZffv2mTNHjx41ZzIyMswZv6yDZkNCQjq0HfeAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMCJbjuMNDMz0zQAb8eOHeZ9REdHmzOSFBpq720/Az8rKirMmT179pgzAwYMMGckqbW11Zy58sorzZmdO3eaM1dccYU5I0kffvihOeNnYKWfYZ8bN240Z1asWGHOSNJDDz1kzvg5Dn5+bv/617+aMzk5OeaM5O9nIzc315zx83M7cuRIc0byN/i0X79+pu07OnyZe0AAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4ES3HUZaW1urxsbGDm9/9dVXm/dx6tQpc0byN0AxOTnZnPEz7NPPgNXt27ebM5L0y1/+0pz5t3/7N3PGzyDJXbt2mTOSNG3aNHPmD3/4gznT0NBgzjQ1NZkzd999tzkj+TsnysrKzBk/g31vvvlmcyYqKsqckaQ1a9aYM5YhymdkZ2ebM/Hx8eaM5O92r08fW1V09Labe0AAACcoIACAE6YCKigo0Lhx4xQVFaX4+HjNmjVLJSUl7bbJzc1VSEhIu8v8+fM7ddEAgJ7PVEBFRUXKz8/Xxo0b9fbbb6ulpUXTpk1TfX19u+3mzZunioqKtsvixYs7ddEAgJ7P9MjS6tWr2328bNkyxcfHa8uWLZo8eXLb5/v166fExMTOWSEAoFf6XI8B1dbWSpJiY2Pbff6FF15QXFycRo0apUWLFl3wGT9NTU2qq6trdwEA9H6+n4bd2tqqBx54QBMnTtSoUaPaPv+1r31N6enpSk5OVnFxsX7wgx+opKREr7322jm/T0FBgR555BG/ywAA9FC+Cyg/P187duzQ+++/3+7z9957b9u/r7rqKiUlJWnq1Knau3evsrKyzvo+ixYt0sKFC9s+rqurU2pqqt9lAQB6CF8FtGDBAr355ptat26dUlJSLrjtmRcR7tmz55wFFAgEFAgE/CwDANCDmQrI8zzdf//9WrFihQoLC5WRkXHRzJlXVCclJflaIACgdzIVUH5+vl588UWtXLlSUVFRqqyslCQFg0H17dtXe/fu1YsvvqibbrpJgwYNUnFxsR588EFNnjxZo0eP7pL/AACgZzIV0DPPPCPp9ItN/96zzz6ruXPnKiIiQu+8846eeuop1dfXKzU1VbNnz9ZPfvKTTlswAKB3MP8J7kJSU1NVVFT0uRYEAPhi6LbTsGNjY01TZSsqKrpwNe3FxcWZMwMHDjRntm7dekn2U1VVZc5I0uOPP27OXHvttebMwYMHzZmIiAhzRpJ+//vfmzPh4eHmjJ9p3WFhYebMvn37zBnJ37RuP4/zlpeXmzN+rttXX33VnJHU7gX2HeXntYx+poL/7//+rzkjSTU1NeaM9Tavo5PbGUYKAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5022GkJSUlpqGD06ZNM+9j/fr15owkTZkyxZzZtm2bOTN27FhzJiQkxJzxMwhRkjIzM82Z/v37mzONjY3mjJ+BsZJ07Ngxc+bGG280Z9asWWPO/OM//qM5c+TIEXNGkvr27WvO7Ny505y5+uqrzRk/76D84IMPmjOSv9uIYDBozvgZsJqWlmbOSP6u2z59bFXR0dsh7gEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnut0sOM/zJEktLS2mXENDg3lfzc3N5ozffTU1NZkzfmag+ZkF5/c4+FnfiRMnuu1+JH/X06U69+rr680Zv8fhzM+hhZ/ryc8MtNbWVnPGz3Uk+Tsf/AgPDzdn/BwHyd/1ZJ0Fd+a4Xew8CvH8nGld6MCBA0pNTXW9DADA51ReXq6UlJTzfr3bFVBra6sOHTqkqKios36br6urU2pqqsrLyxUdHe1ohe5xHE7jOJzGcTiN43BadzgOnufp2LFjSk5OvuC0/W73J7jQ0NALNqYkRUdHf6FPsDM4DqdxHE7jOJzGcTjN9XHoyNtS8CQEAIATFBAAwIkeVUCBQEAPP/ywr3dE7E04DqdxHE7jOJzGcTitJx2HbvckBADAF0OPugcEAOg9KCAAgBMUEADACQoIAOBEjymgJUuWaOjQoYqMjFROTo4+/PBD10u65H72s58pJCSk3WXEiBGul9Xl1q1bp1tuuUXJyckKCQnR66+/3u7rnufpoYceUlJSkvr27au8vDzt3r3bzWK70MWOw9y5c886P2bMmOFmsV2koKBA48aNU1RUlOLj4zVr1iyVlJS026axsVH5+fkaNGiQBgwYoNmzZ6uqqsrRirtGR45Dbm7uWefD/PnzHa343HpEAf3xj3/UwoUL9fDDD2vr1q3Kzs7W9OnTdfjwYddLu+RGjhypioqKtsv777/vekldrr6+XtnZ2VqyZMk5v7548WI9/fTTWrp0qTZt2qT+/ftr+vTpvoYudmcXOw6SNGPGjHbnx0svvXQJV9j1ioqKlJ+fr40bN+rtt99WS0uLpk2b1m5Q64MPPqhVq1bplVdeUVFRkQ4dOqTbb7/d4ao7X0eOgyTNmzev3fmwePFiRys+D68HGD9+vJefn9/28alTp7zk5GSvoKDA4aouvYcfftjLzs52vQynJHkrVqxo+7i1tdVLTEz0Hn/88bbP1dTUeIFAwHvppZccrPDS+Oxx8DzPmzNnjnfrrbc6WY8rhw8f9iR5RUVFnuedvu7Dw8O9V155pW2bnTt3epK8DRs2uFpml/vscfA8z5syZYr37W9/292iOqDb3wNqbm7Wli1blJeX1/a50NBQ5eXlacOGDQ5X5sbu3buVnJyszMxM3X333SorK3O9JKdKS0tVWVnZ7vwIBoPKycn5Qp4fhYWFio+P1/Dhw3Xfffepurra9ZK6VG1trSQpNjZWkrRlyxa1tLS0Ox9GjBihtLS0Xn0+fPY4nPHCCy8oLi5Oo0aN0qJFi3y/LUVX6XbDSD/ryJEjOnXqlBISEtp9PiEhQZ988omjVbmRk5OjZcuWafjw4aqoqNAjjzyi66+/Xjt27FBUVJTr5TlRWVkpSec8P8587YtixowZuv3225WRkaG9e/fqRz/6kWbOnKkNGzYoLCzM9fI6XWtrqx544AFNnDhRo0aNknT6fIiIiFBMTEy7bXvz+XCu4yBJX/va15Senq7k5GQVFxfrBz/4gUpKSvTaa685XG173b6A8P9mzpzZ9u/Ro0crJydH6enpevnll3XPPfc4XBm6gzvvvLPt31dddZVGjx6trKwsFRYWaurUqQ5X1jXy8/O1Y8eOL8TjoBdyvuNw7733tv37qquuUlJSkqZOnaq9e/cqKyvrUi/znLr9n+Di4uIUFhZ21rNYqqqqlJiY6GhV3UNMTIyGDRumPXv2uF6KM2fOAc6Ps2VmZiouLq5Xnh8LFizQm2++qffee6/d27ckJiaqublZNTU17bbvrefD+Y7DueTk5EhStzofun0BRUREaOzYsVq7dm3b51pbW7V27VpNmDDB4crcO378uPbu3aukpCTXS3EmIyNDiYmJ7c6Puro6bdq06Qt/fhw4cEDV1dW96vzwPE8LFizQihUr9O677yojI6Pd18eOHavw8PB250NJSYnKysp61flwseNwLtu3b5ek7nU+uH4WREcsX77cCwQC3rJly7y//vWv3r333uvFxMR4lZWVrpd2SX3nO9/xCgsLvdLSUu+DDz7w8vLyvLi4OO/w4cOul9aljh075m3bts3btm2bJ8l78sknvW3btnn79+/3PM/zfvWrX3kxMTHeypUrveLiYu/WW2/1MjIyvBMnTjheeee60HE4duyY993vftfbsGGDV1pa6r3zzjveNddc411++eVeY2Oj66V3mvvuu88LBoNeYWGhV1FR0XZpaGho22b+/PleWlqa9+6773qbN2/2JkyY4E2YMMHhqjvfxY7Dnj17vJ///Ofe5s2bvdLSUm/lypVeZmamN3nyZMcrb69HFJDned5vf/tbLy0tzYuIiPDGjx/vbdy40fWSLrk77rjDS0pK8iIiIrwhQ4Z4d9xxh7dnzx7Xy+py7733nifprMucOXM8zzv9VOyf/vSnXkJCghcIBLypU6d6JSUlbhfdBS50HBoaGrxp06Z5gwcP9sLDw7309HRv3rx5ve6XtHP9/yV5zz77bNs2J06c8L75zW96AwcO9Pr16+fddtttXkVFhbtFd4GLHYeysjJv8uTJXmxsrBcIBLzLLrvM+973vufV1ta6Xfhn8HYMAAAnuv1jQACA3okCAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATvwfuVu4cmTtpC4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[28, 28, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "HrmyFbhbpHvF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the untrained discriminator to predict whether an image is real or fake\n",
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF3q1Xv_pJxZ",
        "outputId": "12281df0-6487-42d5-94e2-b23ddbff20d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[-0.00018873]], shape=(1, 1), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss and Optimizer"
      ],
      "metadata": {
        "id": "BsbTEuaSpMMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "36POZPcmpONZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discrimnator Loss"
      ],
      "metadata": {
        "id": "oxN-GcD4pSio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "JeNnn1kIpYw7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generator Loss"
      ],
      "metadata": {
        "id": "LYYFG7UlqKYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "metadata": {
        "id": "AsUcChqIqJ_6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "bT8KpdGFqWgn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the checkpoints"
      ],
      "metadata": {
        "id": "6M52qgRwqZ0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "metadata": {
        "id": "Gcl4XrH2qdPz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the training loop"
      ],
      "metadata": {
        "id": "Nas41Dh8qgWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "metadata": {
        "id": "908l7yU6rFIx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "metadata": {
        "id": "n0QJtqVKrHsJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ],
      "metadata": {
        "id": "Un0kuGURrKKs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate and save images"
      ],
      "metadata": {
        "id": "I_bQ8U3msGrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "hTBc3mhWsIbR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "sHgiLYtbsLOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "metadata": {
        "id": "XMnlpPF9sNR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "metadata": {
        "id": "5tVmpp1nsP2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3486962d-93b8-4252-ecb7-6a5264fcc026"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.InitializationOnlyStatus at 0x7b1725eae7a0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a single image using the epoch number\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "metadata": {
        "id": "dOWG4crItnpO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_image(EPOCHS)"
      ],
      "metadata": {
        "id": "_cNyykpdtpHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "f8b0284b-4e96-41a8-e8f3-3a07a108ce6d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'image_at_epoch_0100.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-7d0477526f8f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-2f3d8de0ec11>\u001b[0m in \u001b[0;36mdisplay_image\u001b[0;34m(epoch_no)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display a single image using the epoch number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image_at_epoch_{:04d}.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image_at_epoch_0100.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "metadata": {
        "id": "-jbiVs6Ntq7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(anim_file)"
      ],
      "metadata": {
        "id": "kOCrq8yVtsbx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}